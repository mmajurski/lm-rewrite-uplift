@article{FlashRAG,
	author={Jiajie Jin and
	Yutao Zhu and
	Xinyu Yang and
	Chenghao Zhang and
	Zhicheng Dou},
	title={FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research},
	journal={CoRR},
	volume={abs/2405.13576},
	year={2024},
	url={https://arxiv.org/abs/2405.13576},
	eprinttype={arXiv},
	eprint={2405.13576}
}
@article{ho2020constructing,
	title={Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps},
	author={Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},
	journal={arXiv preprint arXiv:2011.01060},
	year={2020}
}
@article{clark2019boolq,
	title={Boolq: Exploring the surprising difficulty of natural yes/no questions},
	author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1905.10044},
	year={2019}
}
@article{kalyan2021much,
	title={How much coffee was consumed during EMNLP 2019? fermi problems: A new reasoning challenge for AI},
	author={Kalyan, Ashwin and Kumar, Abhinav and Chandrasekaran, Arjun and Sabharwal, Ashish and Clark, Peter},
	journal={arXiv preprint arXiv:2110.14207},
	year={2021}
}
@article{bajaj2016ms,
	title={Ms marco: A human generated machine reading comprehension dataset},
	author={Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and McNamara, Andrew and Mitra, Bhaskar and Nguyen, Tri and others},
	journal={arXiv preprint arXiv:1611.09268},
	year={2016}
}
@article{trivedi2022musique,
	title={MuSiQue: Multihop Questions via Single-hop Question Composition},
	author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
	journal={Transactions of the Association for Computational Linguistics},
	volume={10},
	pages={539--554},
	year={2022},
	publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}
@article{kovcisky2018narrativeqa,
	title={The narrativeqa reading comprehension challenge},
	author={Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
	journal={Transactions of the Association for Computational Linguistics},
	volume={6},
	pages={317--328},
	year={2018},
	publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}




@misc{qwen3technicalreport,
	title={Qwen3 Technical Report}, 
	author={Qwen Team},
	year={2025},
	eprint={2505.09388},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2505.09388}, 
}

@misc{AiPlan,
	title = {America's AI Action Plan},
	url = {https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf},
	langid = {en}
}


@inproceedings{NEURIPS2024_db93ccb6,
	author = {Yu, Yue and Ping, Wei and Liu, Zihan and Wang, Boxin and You, Jiaxuan and Zhang, Chao and Shoeybi, Mohammad and Catanzaro, Bryan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
	pages = {121156--121184},
	publisher = {Curran Associates, Inc.},
	title = {RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/db93ccb6cf392f352570dd5af0a223d3-Paper-Conference.pdf},
	volume = {37},
	year = {2024}
}


@inproceedings{NEURIPS2024_27245589,
	author = {Ru, Dongyu and Qiu, Lin and Hu, Xiangkun and Zhang, Tianhang and Shi, Peng and Chang, Shuaichen and Jiayang, Cheng and Wang, Cunxiang and Sun, Shichao and Li, Huanyu and Zhang, Zizhao and Wang, Binjie and Jiang, Jiarong and He, Tong and Wang, Zhiguo and Liu, Pengfei and Zhang, Yue and Zhang, Zheng},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
	pages = {21999--22027},
	publisher = {Curran Associates, Inc.},
	title = {RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/27245589131d17368cccdfa990cbf16e-Paper-Datasets_and_Benchmarks_Track.pdf},
	volume = {37},
	year = {2024}
}


@article{yang2024crag,
	title={Crag-comprehensive rag benchmark},
	author={Yang, Xiao and Sun, Kai and Xin, Hao and Sun, Yushi and Bhalla, Nikita and Chen, Xiangsen and Choudhary, Sajal and Gui, Rongze and Jiang, Ziran and Jiang, Ziyu and others},
	journal={Advances in Neural Information Processing Systems},
	volume={37},
	pages={10470--10490},
	year={2024}
}




@article{kwiatkowski2019natural,
	title={Natural questions: a benchmark for question answering research},
	author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
	journal={Transactions of the Association for Computational Linguistics},
	volume={7},
	pages={453--466},
	year={2019},
	publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@article{2017arXivtriviaqa,
	author = {{Joshi}, Mandar and {Choi}, Eunsol and {Weld},
	Daniel and {Zettlemoyer}, Luke},
	title = "{triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}",
	journal = {arXiv e-prints},
	year = 2017,
	eid = {arXiv:1705.03551},
	pages = {arXiv:1705.03551},
	archivePrefix = {arXiv},
	eprint = {1705.03551},
}

@misc{pandoc,
	author = {John MacFarlane},
	title = {Pandoc a Universal Document Converter},
	url = {https://pandoc.org/},
	langid = {en}
}

@inproceedings{yang2018hotpotqa,
	title={{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
	author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
	booktitle={Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	year={2018}
}


@article{oh2024generative,
	title={The {G}enerative {AI} paradox on evaluation: What it can solve, it may not evaluate},
	author={Oh, Juhyun and Kim, Eunsu and Cha, Inha and Oh, Alice},
	journal={arXiv preprint arXiv:2402.06204},
	year={2024}
}

@article{yuan2022selecting,
	title={Selecting better samples from pre-trained llms: A case study on question generation},
	author={Yuan, Xingdi and Wang, Tong and Wang, Yen-Hsiang and Fine, Emery and Abdelghani, Rania and Lucas, Pauline and Sauz{\'e}on, H{\'e}l{\`e}ne and Oudeyer, Pierre-Yves},
	journal={arXiv preprint arXiv:2209.11000},
	year={2022}
}

@inproceedings{li2025treeeval,
	title={TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning},
	author={Li, Xiang and Lan, Yunshi and Yang, Chao},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={39},
	pages={24485--24493},
	year={2025}
}

@article{shashidhar2025yourbench,
	title={YourBench: Easy Custom Evaluation Sets for Everyone},
	author={Shashidhar, Sumuk and Fourrier, Cl{\'e}mentine and Lozovskia, Alina and Wolf, Thomas and Tur, Gokhan and Hakkani-T{\"u}r, Dilek},
	journal={arXiv preprint arXiv:2504.01833},
	year={2025}
}
@article{majurski2025generative,
	title={Grounding Generative Evaluations of Language Models in Unsupervised Document Corpora},
	author={Majurski, Michael and Matuszek, Cynthia},
	journal={arXiv preprint arXiv:2505.08905},
	year={2025}
}

@article{alberti2019synthetic,
	title={Synthetic {QA} corpora generation with roundtrip consistency},
	author={Alberti, Chris and Andor, Daniel and Pitler, Emily and Devlin, Jacob and Collins, Michael},
	journal={arXiv preprint arXiv:1906.05416},
	year={2019}
}

@article{shinoda2020improving,
	title={Improving the robustness of {QA} models to challenge sets with variational question-answer pair generation},
	author={Shinoda, Kazutoshi and Sugawara, Saku and Aizawa, Akiko},
	journal={arXiv preprint arXiv:2004.03238},
	year={2020}
}

@article{min2023factscore,
	title={Factscore: Fine-grained atomic evaluation of factual precision in long form text generation},
	author={Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
	journal={arXiv preprint arXiv:2305.14251},
	year={2023}
}
@article{zhang2022tag,
	title={Tag-Set-Sequence learning for generating question-answer pairs},
	author={Zhang, Cheng and Wang, Jie},
	journal={arXiv preprint arXiv:2210.11608},
	year={2022}
}

@article{bai2023benchmarking,
	title={Benchmarking foundation models with language-model-as-an-examiner},
	author={Bai, Yushi and Ying, Jiahao and Cao, Yixin and Lv, Xin and He, Yuze and Wang, Xiaozhi and Yu, Jifan and Zeng, Kaisheng and Xiao, Yijia and Lyu, Haozhe and others},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	pages={78142--78167},
	year={2023}
}

@article{bhat2023investigating,
	title={Investigating answerability of {LLM}s for long-form question answering},
	author={Bhat, Meghana Moorthy and Meng, Rui and Liu, Ye and Zhou, Yingbo and Yavuz, Semih},
	journal={arXiv preprint arXiv:2309.08210},
	year={2023}
}


@article{eo2023towards,
	title={Towards diverse and effective question-answer pair generation from children storybooks},
	author={Eo, Sugyeong and Moon, Hyeonseok and Kim, Jinsung and Hur, Yuna and Kim, Jeongwook and Lee, Songeun and Chun, Changwoo and Park, Sungsoo and Lim, Heuiseok},
	journal={arXiv preprint arXiv:2306.06605},
	year={2023}
}
@article{yang2024qwen2,
	title={Qwen2. 5 technical report},
	author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
	journal={arXiv preprint arXiv:2412.15115},
	year={2024}
}
@article{abdin2024phi,
	title={Phi-4 technical report},
	author={Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J and Javaheripi, Mojan and Kauffmann, Piero and others},
	journal={arXiv preprint arXiv:2412.08905},
	year={2024}
}
@article{team2025gemma,
	title={Gemma 3 technical report},
	author={Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
	journal={arXiv preprint arXiv:2503.19786},
	year={2025}
}
@article{liu2023secqa,
	title={Secqa: A concise question-answering dataset for evaluating large language models in computer security},
	author={Liu, Zefang},
	journal={arXiv preprint arXiv:2312.15838},
	year={2023}
}
@article{grattafiori2024llama,
	title={The llama 3 herd of models},
	author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
	journal={arXiv preprint arXiv:2407.21783},
	year={2024}
}
@misc{llama4,
	title = {The Llama 4 herd: The beginning of a new era of natively multimodal {AI} innovation},
	date = {2024-05},
	url = {https://ai.meta.com/blog/llama-4-multimodal-intelligence/},
	langid = {en}
}

@article{mendez2021current,
	title={Current solutions and future trends for robotic prosthetic hands},
	author={Mendez, Vincent and Iberite, Francesco and Shokur, Solaiman and Micera, Silvestro},
	journal={Annual Review of Control, Robotics, and Autonomous Systems},
	volume={4},
	number={1},
	pages={595--627},
	year={2021},
	publisher={Annual Reviews}
}
@article{chen2025recent,
	title={Recent advances in large language model benchmarks against data contamination: From static to dynamic evaluation},
	author={Chen, Simin and Chen, Yiming and Li, Zexin and Jiang, Yifan and Wan, Zhongwei and He, Yixin and Ran, Dezhi and Gu, Tianle and Li, Haizhou and Xie, Tao and others},
	journal={arXiv preprint arXiv:2502.17521},
	year={2025}
}


@inproceedings{jin2019pubmedqa,
	title = "{P}ub{M}ed{QA}: A Dataset for Biomedical Research Question Answering",
	author = "Jin, Qiao  and
	Dhingra, Bhuwan  and
	Liu, Zhengping  and
	Cohen, William  and
	Lu, Xinghua",
	editor = "Inui, Kentaro  and
	Jiang, Jing  and
	Ng, Vincent  and
	Wan, Xiaojun",
	booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-1259/",
	doi = "10.18653/v1/D19-1259",
	pages = "2567--2577",
}
@article{zhou2024qog,
	title={QOG: Question and Options Generation based on Language Model},
	author={Zhou, Jincheng},
	journal={arXiv preprint arXiv:2406.12381},
	year={2024}
}
@article{zheng2023judging,
	title={Judging llm-as-a-judge with mt-bench and chatbot arena},
	author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	pages={46595--46623},
	year={2023}
}
@article{soboroff2025don,
	title={Don’t use {LLM}s to make relevance judgments},
	author={Soboroff, Ian},
	journal={Information retrieval research journal},
	volume={1},
	number={1},
	pages={10--54195},
	year={2025}
}
@article{auer2024docling,
	title={Docling Technical Report},
	author={Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and others},
	journal={arXiv preprint arXiv:2408.09869},
	year={2024}
}
@misc{UK_AI_Security_Institute_Inspect_AI_Framework_2024,
	author = {{UK AI Security Institute}},
	title = {Inspect {AI:} {Framework} for {Large} {Language} {Model}
	{Evaluations}},
	date = {2024-05},
	url = {https://github.com/UKGovernmentBEIS/inspect_ai},
	howpublished = {\url{https://inspect.ai-safety-institute.org.uk/}},
	year = {2024},
	langid = {en}
}



@article{xu2023expertprompting,
	title={Expertprompting: Instructing large language models to be distinguished experts},
	author={Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong},
	journal={arXiv preprint arXiv:2305.14688},
	year={2023}
}
@inproceedings{szymanski2025limitations,
	title={Limitations of the LLM-as-a-Judge approach for evaluating LLM outputs in expert knowledge tasks},
	author={Szymanski, Annalisa and Ziems, Noah and Eicher-Miller, Heather A and Li, Toby Jia-Jun and Jiang, Meng and Metoyer, Ronald A},
	booktitle={Proceedings of the 30th International Conference on Intelligent User Interfaces},
	pages={952--966},
	year={2025}
}
@inproceedings{yadav-etal-2024-explicit,
	title = "Explicit over Implict: Explicit Diversity Conditions for Effective Question Answer Generation",
	author = "Yadav, Vikas  and
	Kwon, Hyuk joon  and
	Srinivasan, Vijay  and
	Jin, Hongxia",
	editor = "Calzolari, Nicoletta  and
	Kan, Min-Yen  and
	Hoste, Veronique  and
	Lenci, Alessandro  and
	Sakti, Sakriani  and
	Xue, Nianwen",
	booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
	month = may,
	year = "2024",
	address = "Torino, Italia",
	publisher = "ELRA and ICCL",
	url = "https://aclanthology.org/2024.lrec-main.601/",
	pages = "6876--6882",
}
@article{balaguer2024rag,
	title={{RAG} vs fine-tuning: Pipelines, tradeoffs, and a case study on agriculture},
	author={Balaguer, Angels and Benara, Vinamra and Cunha, Renato Luiz de Freitas and Hendry, Todd and Holstein, Daniel and Marsman, Jennifer and Mecklenburg, Nick and Malvar, Sara and Nunes, Leonardo O and Padilha, Rafael and others},
	journal={arXiv preprint arXiv:2401.08406},
	year={2024}
}
@article{wei2024measuring,
	title={Measuring short-form factuality in large language models},
	author={Wei, Jason and Karina, Nguyen and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William},
	journal={arXiv preprint arXiv:2411.04368},
	year={2024}
}
@inproceedings{dua-etal-2019-drop,
	title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
	author = "Dua, Dheeru  and
	Wang, Yizhong  and
	Dasigi, Pradeep  and
	Stanovsky, Gabriel  and
	Singh, Sameer  and
	Gardner, Matt",
	editor = "Burstein, Jill  and
	Doran, Christy  and
	Solorio, Thamar",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1246/",
	doi = "10.18653/v1/N19-1246",
	pages = "2368--2378",
}
@inproceedings{ushio-etal-2023-empirical,
	title = "An Empirical Comparison of {LM}-based Question and Answer Generation Methods",
	author = "Ushio, Asahi  and
	Alva-Manchego, Fernando  and
	Camacho-Collados, Jose",
	editor = "Rogers, Anna  and
	Boyd-Graber, Jordan  and
	Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.899/",
	doi = "10.18653/v1/2023.findings-acl.899",
	pages = "14262--14272",
}
@article{robinson2022leveraging,
	title={Leveraging large language models for multiple choice question answering},
	author={Robinson, Joshua and Rytting, Christopher Michael and Wingate, David},
	journal={arXiv preprint arXiv:2210.12353},
	year={2022}
}

@inproceedings{perez2022discoveringlanguagemodelbehaviors,
	title = "Discovering Language Model Behaviors with Model-Written Evaluations",
	author = "Perez, Ethan  and
	Ringer, Sam  and
	Lukosiute, Kamile  and
	Nguyen, Karina  and
	Chen, Edwin  and
	Heiner, Scott  and
	Pettit, Craig  and
	Olsson, Catherine  and
	Kundu, Sandipan  and
	Kadavath, Saurav  and
	Jones, Andy  and
	Chen, Anna  and
	Mann, Benjamin  and
	Israel, Brian  and
	Seethor, Bryan  and
	McKinnon, Cameron  and
	Olah, Christopher  and
	Yan, Da  and
	Amodei, Daniela  and
	Amodei, Dario  and
	Drain, Dawn  and
	Li, Dustin  and
	Tran-Johnson, Eli  and
	Khundadze, Guro  and
	Kernion, Jackson  and
	Landis, James  and
	Kerr, Jamie  and
	Mueller, Jared  and
	Hyun, Jeeyoon  and
	Landau, Joshua  and
	Ndousse, Kamal  and
	Goldberg, Landon  and
	Lovitt, Liane  and
	Lucas, Martin  and
	Sellitto, Michael  and
	Zhang, Miranda  and
	Kingsland, Neerav  and
	Elhage, Nelson  and
	Joseph, Nicholas  and
	Mercado, Noemi  and
	DasSarma, Nova  and
	Rausch, Oliver  and
	Larson, Robin  and
	McCandlish, Sam  and
	Johnston, Scott  and
	Kravec, Shauna  and
	El Showk, Sheer  and
	Lanham, Tamera  and
	Telleen-Lawton, Timothy  and
	Brown, Tom  and
	Henighan, Tom  and
	Hume, Tristan  and
	Bai, Yuntao  and
	Hatfield-Dodds, Zac  and
	Clark, Jack  and
	Bowman, Samuel R.  and
	Askell, Amanda  and
	Grosse, Roger  and
	Hernandez, Danny  and
	Ganguli, Deep  and
	Hubinger, Evan  and
	Schiefer, Nicholas  and
	Kaplan, Jared",
	editor = "Rogers, Anna  and
	Boyd-Graber, Jordan  and
	Okazaki, Naoaki",
	booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
	month = jul,
	year = "2023",
	address = "Toronto, Canada",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2023.findings-acl.847/",
	doi = "10.18653/v1/2023.findings-acl.847",
	pages = "13387--13434",
	abstract = "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100{\%} of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user`s preferred answer ({\textquotedblleft}sycophancy{\textquotedblright}) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors."
}

@misc{openai_models,
	title={OpenAI Models Documentation},
	year={2025},
	url={https://platform.openai.com/docs/models}, 
}


@article{kim2024biggen,
	title={The biggen bench: A principled benchmark for fine-grained evaluation of language models with language models},
	author={Kim, Seungone and Suk, Juyoung and Cho, Ji Yong and Longpre, Shayne and Kim, Chaeeun and Yoon, Dongkeun and Son, Guijin and Cho, Yejin and Shafayat, Sheikh and Baek, Jinheon and others},
	journal={arXiv preprint arXiv:2406.05761},
	year={2024}
}
@inproceedings{tan-etal-2024-large,
	title = "Large Language Models for Data Annotation and Synthesis: A Survey",
	author = "Tan, Zhen  and
	Li, Dawei  and
	Wang, Song  and
	Beigi, Alimohammad  and
	Jiang, Bohan  and
	Bhattacharjee, Amrita  and
	Karami, Mansooreh  and
	Li, Jundong  and
	Cheng, Lu  and
	Liu, Huan",
	editor = "Al-Onaizan, Yaser  and
	Bansal, Mohit  and
	Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.54/",
	doi = "10.18653/v1/2024.emnlp-main.54",
	pages = "930--957"
}
@inproceedings{han-etal-2024-rag,
	title = "{RAG}-{QA} Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering",
	author = "Han, Rujun  and
	Zhang, Yuhao  and
	Qi, Peng  and
	Xu, Yumo  and
	Wang, Jenyuan  and
	Liu, Lan  and
	Wang, William Yang  and
	Min, Bonan  and
	Castelli, Vittorio",
	editor = "Al-Onaizan, Yaser  and
	Bansal, Mohit  and
	Chen, Yun-Nung",
	booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
	month = nov,
	year = "2024",
	address = "Miami, Florida, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2024.emnlp-main.249/",
	doi = "10.18653/v1/2024.emnlp-main.249",
	pages = "4354--4374"
}
@misc{li2025autobencherdeclarativebenchmarkconstruction,
	title={AutoBencher: Towards Declarative Benchmark Construction}, 
	author={Xiang Lisa Li and Farzaan Kaiyom and Evan Zheran Liu and Yifan Mai and Percy Liang and Tatsunori Hashimoto},
	year={2025},
	eprint={2407.08351},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2407.08351}, 
}
@misc{saadfalcon2024lmunitfinegrainedevaluationnatural,
	title={LMUnit: Fine-grained Evaluation with Natural Language Unit Tests}, 
	author={Jon Saad-Falcon and Rajan Vivek and William Berrios and Nandita Shankar Naik and Matija Franklin and Bertie Vidgen and Amanpreet Singh and Douwe Kiela and Shikib Mehri},
	year={2024},
	eprint={2412.13091},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2412.13091}, 
}
@article{cook2024ticking,
	title={Ticking all the boxes: Generated checklists improve llm evaluation and generation},
	author={Cook, Jonathan and Rockt{\"a}schel, Tim and Foerster, Jakob and Aumiller, Dennis and Wang, Alex},
	journal={arXiv preprint arXiv:2410.03608},
	year={2024}
}
@misc{chiang2023largelanguagemodelsalternative,
	title={Can Large Language Models Be an Alternative to Human Evaluations?}, 
	author={Cheng-Han Chiang and Hung-yi Lee},
	year={2023},
	eprint={2305.01937},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2305.01937}, 
}
@misc{cook2024tickingboxesgeneratedchecklists,
	title={TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation}, 
	author={Jonathan Cook and Tim Rocktäschel and Jakob Foerster and Dennis Aumiller and Alex Wang},
	year={2024},
	eprint={2410.03608},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	url={https://arxiv.org/abs/2410.03608}, 
}
@misc{dubois2025lengthcontrolledalpacaevalsimpleway,
	title={Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators}, 
	author={Yann Dubois and Balázs Galambosi and Percy Liang and Tatsunori B. Hashimoto},
	year={2025},
	eprint={2404.04475},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2404.04475}, 
}
@inproceedings{es2024ragas,
	title={Ragas: Automated evaluation of retrieval augmented generation},
	author={Es, Shahul and James, Jithin and Anke, Luis Espinosa and Schockaert, Steven},
	booktitle={Association for Computational Linguistics: System Demonstrations},
	pages={150--158},
	year={2024}
}
@misc{li2024crowdsourceddatahighqualitybenchmarks,
	title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline}, 
	author={Tianle Li and Wei-Lin Chiang and Evan Frick and Lisa Dunlap and Tianhao Wu and Banghua Zhu and Joseph E. Gonzalez and Ion Stoica},
	year={2024},
	eprint={2406.11939},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2406.11939}, 
}
@misc{tang2024minicheckefficientfactcheckingllms,
	title={MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents}, 
	author={Liyan Tang and Philippe Laban and Greg Durrett},
	year={2024},
	eprint={2404.10774},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2404.10774}, 
}
@misc{malaviya2024expertqaexpertcuratedquestionsattributed,
	title={ExpertQA: Expert-Curated Questions and Attributed Answers}, 
	author={Chaitanya Malaviya and Subin Lee and Sihao Chen and Elizabeth Sieber and Mark Yatskar and Dan Roth},
	year={2024},
	eprint={2309.07852},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2309.07852}, 
}
@inproceedings{fisch-etal-2019-mrqa,
	title = "{MRQA} 2019 Shared Task: Evaluating Generalization in Reading Comprehension",
	author = "Fisch, Adam  and
	Talmor, Alon  and
	Jia, Robin  and
	Seo, Minjoon  and
	Choi, Eunsol  and
	Chen, Danqi",
	editor = "Fisch, Adam  and
	Talmor, Alon  and
	Jia, Robin  and
	Seo, Minjoon  and
	Choi, Eunsol  and
	Chen, Danqi",
	booktitle = "Proceedings of the 2nd Workshop on Machine Reading for Question Answering",
	month = nov,
	year = "2019",
	address = "Hong Kong, China",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D19-5801/",
	doi = "10.18653/v1/D19-5801",
	pages = "1--13"
}
@inproceedings{yang-etal-2018-hotpotqa,
	title = "{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering",
	author = "Yang, Zhilin  and
	Qi, Peng  and
	Zhang, Saizheng  and
	Bengio, Yoshua  and
	Cohen, William  and
	Salakhutdinov, Ruslan  and
	Manning, Christopher D.",
	editor = "Riloff, Ellen  and
	Chiang, David  and
	Hockenmaier, Julia  and
	Tsujii, Jun{'}ichi",
	booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
	month = oct # "-" # nov,
	year = "2018",
	address = "Brussels, Belgium",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/D18-1259/",
	doi = "10.18653/v1/D18-1259",
	pages = "2369--2380"
}














@article{myrzakhan2406open,
	title={Open-{LLM}-{L}eaderboard: From multi-choice to open-style questions for llms evaluation, benchmark, and arena, 2024},
	author={Myrzakhan, Aidar and Bsharat, Sondos Mahmoud and Shen, Zhiqiang},
	journal={URL https://arxiv. org/abs/2406.07545},
	year={2024}
}
@article{balepur2025these,
	title={Which of these best describes multiple choice evaluation with {LLM}s? A) Forced B) Flawed C) Fixable D) All of the Above},
	author={Balepur, Nishant and Rudinger, Rachel and Boyd-Graber, Jordan Lee},
	journal={arXiv preprint arXiv:2502.14127},
	year={2025}
}
@article{balepur2024artifacts,
	title={Artifacts or abduction: How do {LLM}s answer multiple-choice questions without the question?},
	author={Balepur, Nishant and Ravichander, Abhilasha and Rudinger, Rachel},
	journal={Association for Computational Linguistics},
	year={2024}
}
@article{kamalloo2023evaluating,
	title={Evaluating open-domain question answering in the era of large language models},
	author={Kamalloo, Ehsan and Dziri, Nouha and Clarke, Charles LA and Rafiei, Davood},
	journal={Association for Computational Linguistics},
	year={2023}
}
@article{wang2024beyond,
	title={Beyond the answers: Reviewing the rationality of multiple choice question answering for the evaluation of large language models},
	author={Wang, Haochun and Zhao, Sendong and Qiang, Zewen and Qin, Bing and Liu, Ting},
	journal={arXiv e-prints},
	pages={arXiv--2402},
	year={2024}
}
@article{bernard2024equator,
	title={EQUATOR: A Deterministic Framework for Evaluating {LLM} Reasoning with Open-Ended Questions},
	author={Bernard, Raymond and Raza, Shaina and Das, Subhabrata and Murugan, Rahul},
	journal={arXiv preprint arXiv:2501.00257},
	year={2024}
}









@article{wen2024perception,
	title={Perception of knowledge boundary for large language models through semi-open-ended question answering},
	author={Wen, Zhihua and Tian, Zhiliang and Jian, Zexin and Huang, Zhen and Ke, Pei and Gao, Yifu and Huang, Minlie and Li, Dongsheng},
	journal={arXiv preprint arXiv:2405.14383},
	year={2024}
}
@article{zhang2024clamber,
	title={CLAMBER: A benchmark of identifying and clarifying ambiguous information needs in large language models},
	author={Zhang, Tong and Qin, Peixin and Deng, Yang and Huang, Chen and Lei, Wenqiang and Liu, Junhong and Jin, Dingnan and Liang, Hongru and Chua, Tat-Seng},
	journal={arXiv preprint arXiv:2405.12063},
	year={2024}
}















@article{ushio2023empirical,
	title={An empirical comparison of {LM}-based question and answer generation methods},
	author={Ushio, Asahi and Alva-Manchego, Fernando and Camacho-Collados, Jose},
	journal={arXiv preprint arXiv:2305.17002},
	year={2023}
}


@misc{mediumRollingInsight,
	author = {Megan Larsen},
	title = {{R}olling for {I}nsight: {H}ow {G}en {A}{I} and {D}ungeons \& {D}ragons {A}re {B}uilt on {C}ollaboration and {C}reativity --- meganworkmonlarsen.medium.com},
	howpublished = {\url{https://meganworkmonlarsen.medium.com/rolling-for-insight-how-ai-and-dungeons-dragons-are-built-on-collaboration-and-creativity-1f1ee18070e9}},
	year = {2024},
	note = {[Accessed 2024-10-28]},
}

@article{mazeika2024harmbench,
	title={Harmbench: A standardized evaluation framework for automated red teaming and robust refusal},
	author={Mazeika, Mantas and Phan, Long and Yin, Xuwang and Zou, Andy and Wang, Zifan and Mu, Norman and Sakhaee, Elham and Li, Nathaniel and Basart, Steven and Li, Bo and others},
	journal={arXiv preprint arXiv:2402.04249},
	year={2024}
}


@article{andriushchenko2024agentharm,
	title={Agentharm: A benchmark for measuring harmfulness of llm agents},
	author={Andriushchenko, Maksym and Souly, Alexandra and Dziemian, Mateusz and Duenas, Derek and Lin, Maxwell and Wang, Justin and Hendrycks, Dan and Zou, Andy and Kolter, Zico and Fredrikson, Matt and others},
	journal={arXiv preprint arXiv:2410.09024},
	year={2024}
}


@inproceedings{biancini2024multiple,
  title={Multiple-choice question generation using large language models: Methodology and educator insights},
  author={Biancini, Giorgio and Ferrato, Alessio and Limongelli, Carla},
  booktitle={Adjunct Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
  pages={584--590},
  year={2024}
}




@misc{bge_embedding,
	author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
	year={2023},
	title = {HuggingFace BAAI/bge-large-en-v1.5c},
	howpublished = {\url{https://huggingface.co/BAAI/bge-large-en-v1.5}},
	note = {[Accessed 2024-10-28]},
}


@inproceedings{bge_embedding_paper,
	title={C-pack: Packed resources for general chinese embeddings},
	author={Xiao, Shitao and Liu, Zheng and Zhang, Peitian and Muennighoff, Niklas and Lian, Defu and Nie, Jian-Yun},
	booktitle={Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval},
	pages={641--649},
	year={2024}
}


@article{HumanityLastExam,
	title={Humanity's Last Exam},
	author={Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Zhang, Chen Bo Calvin and Shaaban, Mohamed and Ling, John and Shi, Sean and others},
	journal={arXiv preprint arXiv:2501.14249},
	year={2025}
}


@misc{chatGptReuters,
	author = {Sara Merken},
	title = {New York lawyers sanctioned for using fake ChatGPT cases in legal brief},
	howpublished = {\url{https://www.reuters.com/legal/new-york-lawyers-sanctioned-using-fake-chatgpt-cases-legal-brief-2023-06-22/}},
	year = {2023},
	note = {[Accessed 2024-10-28]},
}

@misc{stanfordTrialLegal,
	author = {Varun Magesh and Faiz Surani and Matthew Dahl and Mirac Suzgun and Christopher Manning and Daniel Ho},
	title = {{A}{I} on {T}rial: {L}egal {M}odels {H}allucinate in 1 out of 6 (or {M}ore) {B}enchmarking {Q}ueries --- hai.stanford.edu},
	howpublished = {\url{https://hai.stanford.edu/news/ai-trial-legal-models-hallucinate-1-out-6-or-more-benchmarking-queries}},
	year = {2024},
	note = {[Accessed 2024-10-28]},
}

@misc{forbesLawyerUsed,
	author = {Molly Bohannon},
	title = {{L}awyer {U}sed {C}hat{G}{P}{T} {I}n {C}ourt—{A}nd {C}ited {F}ake {C}ases. {A} {J}udge {I}s {C}onsidering {S}anctions},
	howpublished = {\url{https://www.forbes.com/sites/mollybohannon/2023/06/08/lawyer-used-chatgpt-in-court-and-cited-fake-cases-a-judge-is-considering-sanctions/}},
	year = {2023},
	note = {[Accessed 2024-10-28]},
}



@inproceedings{DBLP:journals/corr/abs-1806-03822,
	title = "Know What You Don`t Know: Unanswerable Questions for {SQ}u{AD}",
	author = "Rajpurkar, Pranav  and
	Jia, Robin  and
	Liang, Percy",
	editor = "Gurevych, Iryna  and
	Miyao, Yusuke",
	booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
	month = jul,
	year = "2018",
	address = "Melbourne, Australia",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P18-2124/",
	doi = "10.18653/v1/P18-2124",
	pages = "784--789",
	abstract = "Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."
}

@article{DBLP:journals/corr/abs-2009-03300,
  author       = {Dan Hendrycks and
                  Collin Burns and
                  Steven Basart and
                  Andy Zou and
                  Mantas Mazeika and
                  Dawn Song and
                  Jacob Steinhardt},
  title        = {Measuring Massive Multitask Language Understanding},
  journal      = {CoRR},
  volume       = {abs/2009.03300},
  year         = {2020},
  url          = {https://arxiv.org/abs/2009.03300},
  eprinttype    = {arXiv},
  eprint       = {2009.03300},
  timestamp    = {Thu, 17 Sep 2020 12:49:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2009-03300.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{evidentlyaiWrongUseful,
	author = {Elena Samuylova},
	title = {{W}rong but useful: an {L}{L}{M}-as-a-judge tutorial},
	howpublished = {\url{https://www.evidentlyai.com/blog/llm-as-a-judge-tutorial}},
	year = {2024},
	note = {[Accessed 29-10-2024]},
}


@misc{huggingfaceUsingLLMasajudge,
	author = {Aymeric Roucher},
	title = {{U}sing {L}{L}{M}-as-a-judge for an automated and versatile evaluation - {H}ugging {F}ace {O}pen-{S}ource {A}{I} {C}ookbook},
	howpublished = {\url{https://huggingface.co/learn/cookbook/en/llm_judge}},
	year = {2024},
	note = {[Accessed 2024-10-28]},
}

@misc{substackUsingLLMs,
	author = {Cameron Wolfe},
	title = {{U}sing {L}{L}{M}s for {E}valuation},
	howpublished = {\url{https://cameronrwolfe.substack.com/p/llm-as-a-judge}},
	year = {2024},
	note = {[Accessed 2024-10-28]},
}


@misc{confidentaiEvaluationMetrics,
	author = {Jeffrey Ip},
	title = {{L}{L}{M} {E}valuation {M}etrics: {T}he {U}ltimate {L}{L}{M} {E}valuation {G}uide},
	howpublished = {\url{https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation}},
	year = {2024},
	note = {[Accessed 2024-10-28]},
}


@misc{huggingfaceMetallamaLlama321BHugging,
	author = {},
	title = {meta-llama/{L}lama-3.2-1{B} · {H}ugging {F}ace --- huggingface.co},
	howpublished = {\url{https://huggingface.co/meta-llama/Llama-3.2-1B}},
	year = {2024},
	note = {[Accessed 2024-10-28]},
}

@misc{metaLlama32,
	author = {},
	title = {{L}lama 3.2: {R}evolutionizing edge {A}{I} and vision with open, customizable models},
	howpublished = {\url{https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/}},
	year = {2024},
	note = {[Accessed 2024-10-28]},
}

@misc{aisiEarlyInsights,
	author = {},
	title = {{E}arly {I}nsights from {D}eveloping {Q}uestion-{A}nswer {E}valuations for {F}rontier {A}{I} | {A}{I}{S}{I}},
	howpublished = {\url{https://www.aisi.gov.uk/work/early-insights-from-developing-question-answer-evaluations-for-frontier-ai}},
	year = {2024},
	note = {[Accessed 2024-10-28]},
}





@article{DBLP:journals/corr/abs-1803-05457,
  author       = {Peter Clark and
                  Isaac Cowhey and
                  Oren Etzioni and
                  Tushar Khot and
                  Ashish Sabharwal and
                  Carissa Schoenick and
                  Oyvind Tafjord},
  title        = {Think you have Solved Question Answering? Try ARC, the {AI2} Reasoning
                  Challenge},
  journal      = {CoRR},
  volume       = {abs/1803.05457},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.05457},
  eprinttype    = {arXiv},
  eprint       = {1803.05457},
  timestamp    = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-05457.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}