\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\PassOptionsToPackage{numbers}{natbib}
\usepackage[preprint]{acl}
%\usepackage[square,numbers]{natbib}


% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}



\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}       
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors


\usepackage[inline]{enumitem}
\usepackage{makecell}
\usepackage{fancyhdr}       % header
\usepackage{subcaption}
\usepackage{mdframed}
\usepackage{comment}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{placeins}
\usepackage{float}
\usepackage{array} % for centering column content  
\usepackage{wrapfig}
\usepackage{cleveref}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{atbeginend}
%\usepackage{pifont}

\newenvironment{packed_enum}{
\begin{enumerate}
	\setlength{\itemsep}{1pt}
	\setlength{\parskip}{0pt}
	\setlength{\parsep}{0pt}
}{\end{enumerate}}

\newenvironment{packed_item}{
\begin{itemize}
	\setlength{\itemsep}{1pt}
	\setlength{\parskip}{0pt}
	\setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packed_description}{
\begin{description}
	\setlength{\itemsep}{1pt}
	\setlength{\parskip}{0pt}
	\setlength{\parsep}{0pt}
}{\end{description}}

\AfterBegin{packed_enum}{\vspace{-0.7em}}
\AfterEnd{packed_enum}{\vspace{-0.7em}}
\AfterBegin{packed_item}{\vspace{-0.7em}}
\AfterEnd{packed_item}{\vspace{-0.7em}}
\AfterBegin{packed_description}{\vspace{-0.7em}}
\AfterEnd{packed_description}{\vspace{-0.7em}}

\AfterBegin{quote}{\vspace{-0.7em}}
\AfterEnd{quote}{\vspace{-0.7em}}

\lstdefinelanguage{json}{
basicstyle=\ttfamily,
numbers=left,
numberstyle=\tiny\color{gray},
stepnumber=1,
numbersep=8pt,
showstringspaces=false,
breaklines=true,
%	frame=lines,
%	backgroundcolor=\color{lightgray},
literate=
*{0}{{{\color{blue}0}}}{1}
{1}{{{\color{blue}1}}}{1}
{2}{{{\color{blue}2}}}{1}
{3}{{{\color{blue}3}}}{1}
{4}{{{\color{blue}4}}}{1}
{5}{{{\color{blue}5}}}{1}
{6}{{{\color{blue}6}}}{1}
{7}{{{\color{blue}7}}}{1}
{8}{{{\color{blue}8}}}{1}
{9}{{{\color{blue}9}}}{1}
{:}{{{\color{red}:}}}{1}
{,}{{{\color{red},}}}{1}
{\{}{{{\color{orange}\{}}}{1}
{\}}{{{\color{orange}\}}}}{1}
{[}{{{\color{orange}[}}}{1}
{]}{{{\color{orange}]}}}{1},
}

\lstdefinelanguage{txt}{
basicstyle=\ttfamily,
numbers=left,
numberstyle=\tiny\color{gray},
stepnumber=1,
numbersep=8pt,
showstringspaces=false,
breaklines=true,
%	frame=lines,
%	backgroundcolor=\color{lightgray},
}



%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
%\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
aboveskip=0pt,belowskip=0pt,%
showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

\setcounter{secnumdepth}{3} %May be changed to 1 or 2 if section numbers are desired.

%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Title

%\title{Evaluating LM Accuracy Uplift From Rewriting Questions to Remove Ambiguities}
%\title{Evaluating LM Accuracy Uplift: Using Answer Free Context to Double gpt-oss-20b Performance on a subset of HLE}
\title{Evaluating LM Accuracy Uplift: Doubling the Performance on Humanities Last Exam Using Answer-Free Context}
%\title{Evaluating LM Context Uplift: Doubling HLE Benchmark Performance Using Answer-Free Context}
%\title{LM Accuracy Uplift: Using Answer-Free Context Information to Double gpt-oss-20b Performance on a subset of HLE}
%\title{Squeezing All Improvement From Your Context: QA Accuracy Uplift Even When the Context Doens't Surface the Answer}



\author{%
Michael Majurski$^{12*}$ \quad Cynthia Matuszek$^{2}$\\
$^1$National Institute of Standards and Technology \quad $^2$University of Maryland Baltimore County\\
\texttt{michael.majurski@nist.gov}\\
\texttt{cmat@umbc.edu}\\
}





\begin{document}

\maketitle
% gpt-5-mini on HLE rewrite using gpt-oss-120b
% orig = 0.073
% rewrite = 0.439

% gpt-5-mini on HLE rewrite using gpt-oss-20b
% orig = 0.139
% rewrite = 0.372

\begin{abstract}
	How eloquently, clearly, and unambiguously one poses a question has a stark impact on the quality of answer.
	This concept holds for knowledge questions posed to Language Models (LMs). 
	LMs continue to advance and a plethora of benchmarks have been developed (at great expense) to assess model capability. 
	This work studies the interplay between background grounding information presented to the LM in context with question quality and ambiguity.
	We fine that combining well grounded dynamic context construction (i.e. RAG) with query rewriting to reduce ambiguity results in significantly improved model accuracy.
	Specifically, given a user question with associated grounding context that does not contain the answer, benchmark accuracy uplift is significant, even compared to just prepending that context before the question.
	Using \texttt{gpt-oss-20b} to rewrite a subset of Humanities Last Exam using answer-free grounding context improves \texttt{gpt-5-mini} accuracy from 0.14 to 0.37.
	This uplift in accuracy cannot be fully recovered just through prompting at question evaluation time, separate rewriting and answering phases are required.
	Additionally, with LM benchmarking we demonstrate that question rewriting improves the embedding space alignment between the question and grounding context compared to the original.
	% TODO fill in before final camera ready
	%	Code is available at \url{https://github.com/mmajurski/grounded-synth-lm-benchmark}
\end{abstract}



\section{Introduction}




%Intro to reword:
%Users often implicitly assume that an LLM shares their mental model, including their background knowledge, context, and intent. This leads them to omit critical information when formulating a query, believing it to be self-evident. The LLM, however, operates on the statistical patterns in its training data and the explicit text of the prompt. When faced with an underspecified query, it must make an assumption to generate a response. If the model's chosen assumption does not align with the user's unstated intent, the resulting answer—while potentially factually correct under that assumed interpretation—will be perceived by the user as incorrect, irrelevant, or even as a "hallucination".

The ongoing explosion of Language Model (LM) capability is largely a consequence of scaling laws, which have demonstrated a relationships between performance and parameter count, dataset size, and compute. 
This capability growth has opened a widening chasm between what modern LMs can do and how we measure their abilities.
Static benchmarking is a brittle Gold Standard.
Leaderboard style benchmarks have been the primary engine of progress in machine learning, providing an objective scalable way to measure and compare capabilities. 
%However, their static nature has become their greatest vulnerability. 
%In the current ecosystem of web-scale data and intense competitive pressure, these benchmarks are proving to be a brittle and increasingly unreliable standard, suffering from a host of interconnected failure modes that distort our understanding of AI progress.
\begin{figure}[t!]
	%	\vspace*{-1.0em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/fig1.jpg}
	\vspace{-0.5em}
	\caption{When RAG systems surface relevant information but not the answer, LM performance can be enhanced by disambiguating the question using context.}
	\label{fig:figure1}
	\vspace{-1em}   
\end{figure}
\begin{figure}[b!]
	\vspace*{-1.0em}
	\centering
	\includegraphics[width=0.8\columnwidth]{figs/gpt20b-afc_hle.pdf}
	\vspace{-0.5em}
	\caption{Rewriting questions 
		%using \texttt{gpt-oss-20b} 
		using answer-free grounding context yields significant accuracy improvement over the original questions as evaluated on a subset of Humanities Last Exam (HLE).
		%Questions rewritten by gpt-oss-20b using answer-free grounding context information demonstrate significant benchmark accuracy improvement over the original questions as evaluated on a subset of Humanities Last Exam (HLE) where validated context information is available. 
	}
	\label{fig:hle-uplift}
	%	\vspace{-1em}   
\end{figure}
LMs undeniably possess considerable general knowledge; but are inherently weaker with domain-specific and proprietary information. 
Specifically, with AI systems targeting professional workflows, new types of evaluation are required to characterize LM capability grounded in document populations.
For enterprise applications with internal corporate knowledge management the generalized information contained within a pre-trained LM is insufficient. 
These domains require access to private, specialized, and often confidential data that is not part of any public training corpus. 
Current solution approaches involve retrieval systems which dynamically construct context for the LM based upon user queries~\cite{NEURIPS2024_db93ccb6}.
Ongoing evaluation of Retrieval-Augmented Generation (RAG) systems focuses heavily on ranking quality and retrieval performance~\cite{yang2024crag,NEURIPS2024_27245589} with less focus on how to best utilize the retrieved information.
LM performance evaluation is accurate if RAG surfaces the correct answer, but if it only retrieves relevant information (without the answer), performance suffers. 
\Cref{fig:figure1} outlines our methodology for extracting both additional benchmark accuracy and improved grounding via question disambiguation rewrite using answer-free context information.
\Cref{fig:hle-uplift} demonstrates the improvement this method provides on a subset of Humanities Last Exam with validated grounding context, showing \texttt{gpt-5-mini} improving from an accuracy of 0.14 on the original questions to 0.37 on the rewritten questions.

To highlight the core problem this paper addresses, \Cref{fig:impact-of-context} highlights the impact of context on LM benchmark performance (across all datasets used in this study). 
Asking \texttt{gpt-oss-120b} a question without any supporting context produces mixed results as your relying on the model weight knowledge (blue).
\texttt{Question+Context} (red), with supporting context containing the desired answer performs well.
Answer-Free Context (\texttt{AFC}) is background grounding information relevant to the question but which does not contain the answer. 
LM performance under \texttt{Question+AFC} (green) cannot match the context supported accuracy.
\begin{figure}[t!]
	\vspace*{-1.5em}
	\centering
	\includegraphics[width=0.75\columnwidth]{figs/impact_of_context_gpt-oss-120b.pdf}
	\vspace{-0.8em}
	{\small
		% \caption{The quality of context information presented to the LM has a drastic impact on performance. If RAG systems surface Context with the answer, benchmark accuracy numbers improve drastically compared to just asking the question. However, if the surfaced Context is Answer-Free, there is minimal utility to include it alongside the question being asked.}
		\caption{The quality of context information presented to the question answering LM has a drastic impact on system performance. Accuracy is high when RAG systems correctly surface context with the answer (red), but when the question is presented without context (blue) or the surfaced information does not contain the answer (green), benchmark performance suffers.}
		\label{fig:impact-of-context}
	}	
	\vspace{-1.0em}   
	
\end{figure}
However, as shown in \Cref{fig:hle-uplift}, using \texttt{AFC} to rewrite the question can both disambiguate what is being asked and fill in relevant background assumptions, producing significant accuracy gains. 
This increase in performance does not rely on model weight knowledge (gpt-oss-20b is an adequate re-writer).
Additionally, just prepending the \texttt{AFC} text before the question does not produce an equivalent accuracy uplift as shown in \Cref{fig:impact-of-rewrite} where evaluating the \texttt{Question+Answer-Free Context} under-performs the rewritten question.
The act of interpretation during rewriting to clarify and disambiguate the question separately from attempting to answer it causes the uplift effect. 
\begin{figure}[t!]
	\vspace*{-1.5em}
	\centering
	\includegraphics[width=0.75\columnwidth]{figs/qAFC_vs_RQ_gpt120b.pdf}
	\vspace{-0.8em}
	{\small
		\caption{The act of interpretation during question rewriting produces an accuracy uplift beyond just prepending the \texttt{Answer-Free Context} information used to rewrite before the question.}
		\label{fig:impact-of-rewrite}
	}
	\vspace{-1.0em}   
\end{figure}
An intuition behind this result is asking a student to restate the question before answering it.
The act of puzzling through what is being asked enhances understanding of how to correctly answer the question.








\section{Related Works}


% maybe talk in related works about the disconnect between HLE type benchmarks where the model just has to answer, and when tool use (i.e. internet) is available. In modern RAG finding and surfacing relevant and on topic informatino about the question can provide significant alpha. But that requires shifrting the evaluation target from a Model (+tools) to the Model+RAG dynamic context engine system. That second more complex system with grounding documents is much harder to evaluate than an LM in isolation. 


While LMs store significant parametric knowledge, their responses are not grounded. %without additional components their responses are not grounded in reliable external information source.
Dynamic context construction approaches like RAG address this by fetching external evidence during the generation process \cite{sharma2025retrieval}.
Grounding the LM response in external (non-parametric) data significantly improves factual accuracy \cite{sharma2025retrieval}.
The RAG pipeline introduces its own optimization challenges, spanning document chunking, search, ranking \cite{NEURIPS2024_db93ccb6}, and post-retrieval processing \cite{dai2025evinote}.
A particularly critical component, with a long history in information retrieval, is the transformation of the users query \cite{Rastogi2019,rivas2014study}.

The initial user query is often an imperfect expression of the underlying information need, suffering from ambiguity, missing context, or terminology poorly aligned with the target document corpus. 
To bridge this lexical gap, modern RAG systems, like their classic predecessors, employ query expansion and rewriting \cite{jagerman2023query,zhou2023unified}. 
The goal is to reformulate the query to better surface relevant documents and improve the signal-to-noise ratio of the retrieved context.

LMs have proven adept at this rewriting, expansion, and contextualization task; disambiguating queries using carefully designed prompts \cite{wilson2025contextualizing}. 
Other approaches explore post RAG retrieval optimizations for improving signal to noise in the surfaced documents, shifting from a retrieve-generate paradigm to retrieve-note-generate where the discovered documents are summarized into high level notes \cite{dai2025evinote}.

Other prompting strategies focus on getting the most out of the documents placed into the LM context.
For example, Step-Back Prompting attempts to have LMs derive high-level concepts from first principles to guide reasoning in query answering \cite{zheng2023take}.
Frameworks like AGREE formulate methods to have the LM cite its sources within the context window (drawn from grounding documents) \cite{ye2023effective}.
Other approaches involve extensions to chain-of-thought including Meta-CoT which models how to determine what underlying reasoning is required to arrive at a particular CoT \cite{xiang2025towards}.
Any CoT type reasoning approaches enable the LM employ some form of question rewriting during the process of figuring out how best to respond.

While benchmarks exists for end-to-end LM systems with RAG \cite{NEURIPS2024_27245589,yang2024crag}, they can suffer from the same static benchmarking flaws LM knowledge evaluations have. 
A primary cause of benchmark utility decay is the public and static nature of the test data. 
Shifting to dynamic generative benchmarking constructed on-demand from trusted document corpora is one solution.
This approach, exemplified by \cite{majurski2025generative,shashidhar2025yourbench,li2025autobencherdeclarativebenchmarkconstruction}, extends the evaluation landscape by making it possible to create private, domain-specific, and temporally relevant assessments that are inherently resistant to contamination.


%This paradigm empowers users to move beyond generic, one-size-fits-all benchmarks and create evaluations tailored to their specific needs; enabling  more accurate assessment of a model's utility for specific applications.
%For example, a law firm can generate a benchmark from its case files, a pharmaceutical company can test a model's knowledge of its latest research papers, and a software company can evaluate a model's ability to understand its proprietary codebase. 
A central tenet of this generative evaluation approach is benchmarks must be grounded in the provided source documents. 
This means that each question should be verifiably answerable using only the information contained in the source text. 
This grounding is critical because it shifts the object of evaluation from "what the model knows" (parametric knowledge) to "how the model reasons" over a given context. 
Combined with varying levels of compute budget, benchmark-free methods like TreeEval~\cite{li2025treeeval} might serve dynamic context construction LM evaluation well, enabling an adaptive powerful "examiner" LM tasked with probing the target model for weaknesses given a topic. 
TreeEval starts by generating initial questions, then dynamically generating follow up based on the initial response building a tree of inquiry, potentially using information sources the model under evaluation does not have access to, enhancing the evaluation asymmetry. 
Combining generative benchmarking with document grounding evaluation critically important for next-gen LM system evaluation approaches. 



%Literature Survey Notes for Acc Uplift
%
%
%# RAG
%
%- General survey of RAG https://arxiv.org/pdf/2506.00054v1
%- "CRAG-comprehensive rag benchmark"
%- https://proceedings.neurips.cc/paper_files/paper/2024/file/27245589131d17368cccdfa990cbf16e-Paper-Datasets_and_Benchmarks_Track.pdf
%- https://proceedings.neurips.cc/paper_files/paper/2024/file/db93ccb6cf392f352570dd5af0a223d3-Paper-Conference.pdf
%
%## RAG Query Expansion/Rewriting
%From LM:
%"The initial query provided by a user is often an imperfect expression of their underlying information need. It may be ambiguous, lack critical context, or use terminology that does not align with the target document corpus. A long-standing principle in information retrieval holds that transforming this initial query can lead to significant improvements in system performance. In the age of LLMs, this principle has been revitalized, with a growing body of research exploring the use of language models themselves to rewrite, expand, and refine queries before they are passed to a retrieval system."
%
%"The concept of modifying a user's query is not new. Classic information retrieval systems have long employed techniques like query expansion (QE), the process of reformulating a query to improve retrieval performance.1 This is often done by adding synonyms, related terms, or terms from pseudo-relevance feedback (i.e., terms from the top-retrieved documents of an initial search) to the original query, with the goal of increasing recall by bridging the lexical gap between the user's vocabulary and the corpus. Over time, these methods evolved into more sophisticated query reformulation techniques that combine syntactic and semantic information to suggest more helpful alternatives to the user, helping them avoid "thrashing" on minor variations of an unsuccessful query."
%
%Papers:
%- This paper explores query expansion https://arxiv.org/html/2404.07221v2
%- This paper explores query rewriting https://arxiv.org/pdf/2502.15009v1
%- query rewriting for medical https://www.mdpi.com/2078-2489/12/10/402
%- LM aided rewriting https://aclanthology.org/2023.findings-emnlp.398/
%- Contextual Query Rewriting https://aclanthology.org/2023.acl-industry.58.pdf
%- Contextual query rewriting https://www.amazon.science/publications/contextual-query-rewriting-cqr-natural-language-as-interface-for-dialog-state-tracking
%- This paper does a compress step to improve rag answering performance turning retrive->answer into retrive->note->answer https://arxiv.org/html/2509.00877v1
%
%## RAG Grounding
%- https://arxiv.org/html/2409.11242v1
%"In this work, we propose Trust-Score—a novel holistic metric to exclusively evaluate the trustworthiness of LLMs for RAG. Trust-Score assesses an LLM across multiple dimensions: 1) The ability to discern which questions can be answered or refused based on the provided documents (Grounded Refusals);"
%
%
%# Grounding In Documents
%
%- structured prompting for long legal documents https://arxiv.org/pdf/2509.02241v1
%- AGREE grounding framework https://arxiv.org/pdf/2311.09533v3 
%
%
%
%# CoT improvements for accuracy
%- MetaCoT for getting the model to figure out how to answer https://arxiv.org/pdf/2501.04682 this did not work under our testing, at least for the smaller less capable models. The separate rewrite and the evaluate tasks produced higher accuracy
%- StepBack Prompting to improve reasoning performance https://arxiv.org/pdf/2310.06117




%The evaluation of large language models (LLMs) is confronting a crisis of methodology, as the static benchmarks that once drove progress are now proving to be fundamentally unreliable.  A primary challenge is data contamination, where public benchmark datasets are inevitably absorbed into the web-scale corpora used for training subsequent models, leading to inflated scores that reflect memorization rather than true generalization. 
%This issue is compounded by benchmark saturation, where top models achieve near-perfect scores, rendering the tests incapable of differentiating between state-of-the-art systems.  The intense focus on leaderboards has also triggered Goodhart's Law, where the metric becomes the target, incentivizing developers to "overtune" models to exploit benchmark artifacts instead of pursuing genuine, robust capabilities.  Furthermore, these models exhibit a profound structural fragility, with performance collapsing in response to semantically irrelevant changes like rephrasing questions or reordering multiple-choice options, indicating a failure to learn underlying problem structures.   
%
%In response to these failings, the research community has shifted towards generative and dynamic evaluation paradigms. 
%This has culminated in the development of dynamic, adversarial frameworks where an examiner LLM interacts with the model under evaluation. 
%These systems, such as TreeEval, create an "irreproducible evaluation session" by adaptively generating follow-up questions based on the model's responses, making it impossible to "study for the test" and enabling a more authentic assessment of on-the-fly reasoning \cite{bai2023benchmarking, li2025treeeval}.
%
%A crucial advancement in synthetic benchmark generation is the principle of grounding, which connects the evaluation to a trusted, external corpus of documents.  By requiring that all questions be verifiably answerable from the provided source material, grounding shifts the assessment from what a model has memorized (parametric knowledge) to how it reasons over a given context.  This approach is exemplified by modern frameworks like YourBench, which can synthetically replicate benchmarks like MMLU from a small set of source documents, and AutoBencher, which uses privileged information to discover knowledge gaps \cite{li2025autobencherdeclarativebenchmarkconstruction, shashidhar2025yourbench}. 
%This methodology is not new, with earlier benchmarks like SecQA and PubMedQA also being grounded in domain-specific texts \cite{jin2019pubmedqa, liu2023secqa}.  As modern long-context models can now process entire documents, full-document grounding has proven superior to earlier summary-based techniques, further enhancing the quality and reliability of the generated evaluations \cite{bhat2023investigating}. 
%Ultimately, grounding improves the utility of synthetic data by ensuring that the evaluation is novel, domain-specific, and a true test of reasoning rather than recall.
%
%Grounding Large Language Models (LLMs) with external documents via Retrieval-Augmented Generation (RAG) has become a primary strategy for mitigating factual inaccuracies and knowledge cutoffs. However, building and evaluating these systems presents significant challenges, foremost among them being the dependency on retrieval quality. The principle of "garbage in, garbage out" dictates that the entire system's performance is fundamentally limited by the retriever's ability to surface relevant, accurate, and complete information. Research has identified numerous failure points within this retrieval stage, including cases where the correct information exists in the knowledge base but is not ranked highly enough to be included in the context, or where the information is missing from the source entirely. This retrieval bottleneck is compounded by the inherent complexity of the RAG pipeline, which involves a series of interdependent components—from data ingestion and chunking strategies to the choice of retrieval algorithm (e.g., dense, sparse, or hybrid search)—each requiring careful tuning and maintenance.   
%
%Beyond the initial retrieval, significant challenges persist in how the LLM utilizes the provided context. Even when relevant documents are successfully retrieved, their effectiveness can be undermined by the "lost in the middle" problem, a phenomenon where LLMs exhibit a strong bias towards information at the beginning and end of a long context window, effectively ignoring relevant facts buried in the middle. This issue, rooted in the transformer architecture's positional biases and attention dilution, challenges the naive assumption that providing more context is always beneficial. Furthermore, deploying RAG systems introduces systemic hurdles such as increased latency and computational cost compared to standalone LLM calls, creating practical trade-offs between response quality, speed, and expense in production environments. These systems also struggle with complex, multi-hop questions that require synthesizing information from multiple documents, a task for which a single retrieval pass is often insufficient.   
%
%Addressing these multifaceted challenges requires robust evaluation methodologies. The research community has largely adopted a bifurcated approach, assessing the retrieval and generation components of RAG systems independently to better diagnose failures. Retrieval quality is typically measured using classic information retrieval metrics like Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG). The quality of the final generated response is evaluated on dimensions such as faithfulness (whether the answer is supported by the context) and relevance to the query, often using another powerful LLM as an automated judge. To standardize these evaluations, a number of key benchmarks have been established in related works, including Natural Questions (NQ) for open-domain question answering, HotpotQA for multi-hop reasoning, and more recent, specialized benchmarks like LaRA, which compares RAG with long-context models, and mmRAG for multi-modal retrieval. The development of comprehensive evaluation frameworks such as Auepora and BERGEN further signals a push towards more reproducible and systematic analysis of RAG performance across its many configurations.


\section{Methods}

This work explores applying a RAG query rewrite/expansion to produce accuracy uplift on questions posed to LMs.
%This work explores the impact context can have of informational questions being asked of LM systems. 
Prior work in RAG systems has demonstrated the utility of query expansion~\cite{wilson2025contextualizing} and prior wok in generative benchmarking demonstrated a correlation between generated question length and model benchmark accuracy~\cite{majurski2025generative}.
%Prior work in the field demonstrated a correlation between generative benchmarking question length and model performance~\cite{majurski2025generative}.
These two pieces of evidence indicate that applying rewriting to knowledge benchmarks would produce an uplift in accuracy by placing the LM under evaluation in the right "state of mind" to answer the question better.
This work explores that trend and characterizes how well that effect generalizes, and provides some thoughts on why it might happen. 

\subsection{Datasets \& Models}

To explore the impact of question rewriting on LM benchmark performance, pairs of questions with associated context that provides the correct answer are required.
While most RAG systems are designed to produce that type of information, validated reference data like that is in short supply. 
Additionally, datasets from post model knowledge cutoff enables evaluating whether the observed effects stem from memorization/recall within the model weights, or whether the question disambiguation approach generalizes to new unseen knowledge. 

\Cref{tab:dataset_publication_cutoff} indicates which datasets were used in this study, and highlights the release data of the benchmark. 
Only Humanities Last Exam (HLE)~\cite{HumanityLastExam} and the generative benchmarking data was new enough to be post knowledge cutoff for all models tested.

\begin{table}[h!]
	\centering
	\caption{Evaluation Datasets and Publication Dates}
%	\footnotesize
	\scriptsize
	\label{tab:dataset_publication_cutoff}
	\vspace{-1em}   
	
	\begin{tabular}{p{5.5cm} p{1.25cm}}
		\toprule
		\textbf{Dataset} & \textbf{Release Date}  \\
		Public Datasets & \\
		\toprule
		Humanities Last Exam~\cite{HumanityLastExam} & Jan 2025\\
		\hline
		Squadv2~\cite{DBLP:journals/corr/abs-1806-03822} & 2018 \\
		\hline
		HotpotQA~\cite{yang2018hotpotqa} & 2018 \\
		\hline
		TrivaQA-web~\cite{2017arXivtriviaqa} & 2017 \\
		\hline
		NaturalQuestionsShort~\cite{kwiatkowski2019natural} & 2019 \\
		\hline
		PubMedQA~\cite{jin2019pubmedqa} & 2019 \\
		\hline
		BoolQ~\cite{clark2019boolq} & 2019 \\
		\hline
		FermiQA~\cite{kalyan2021much} & 2021 \\
		\hline
		MS-MARCO-QA~\cite{bajaj2016ms} & 2016 \\
		\hline
		MusiqueQA~\cite{trivedi2022musique} & 2022 \\
		\hline
		2WikiMultiHopQA~\cite{ho2020constructing} & 2020 \\
		\bottomrule
		\vspace{0.1em}   
		Generative Benchmarks (constructed using~\cite{majurski2025generative}) & \\
		\toprule
		arXiv\_2502\_17521v1~\cite{chen2025recent} & 2025 \\
		\hline
		America's AI Action Plan~\cite{AiPlan} & 2025 \\
		\bottomrule
		\vspace{0.1em}   
		Generative Benchmarks (constructed using~\cite{shashidhar2025yourbench}) & \\
		\toprule
		arXiv\_2502\_17521v1~\cite{chen2025recent} & 2025 \\
		\hline
		America's AI Action Plan~\cite{AiPlan} & 2025 \\
		\bottomrule
	\end{tabular}
	\vspace{-1em} 
\end{table}

% HLE data sourceing https://www.futurehouse.org/research-announcements/hle-exam
% Audited HLE dataset https://huggingface.co/datasets/futurehouse/hle-gold-bio-chem
The originally published HLE dataset does not contains associated context with each question, however FutureHouse released manual validation for a subset of chemistry/biology questions with grounding literature~\cite{HleFutureHouse}.
For our work, we only used the subset of HLE questions FutureHouse annotated as correctly answered and which had sufficient grounding information.

Evaluating the efficacy of query rewriting for LM benchmark evaluation requires two categories of models. 
First the model doing the question rewriting, note this is not the same as the model under evaluation on the benchmark.
We separate the question rewriting and the evaluation phases to ensure that all evaluation models are given the same rewritten questions to reduce system measurement noise. 
Second are the set of models we evaluated using the rewritten questions; these span a wide range of size and capability.
\Cref{tab:model_cutoff} outlines the models we evaluated the rewritten question benchmarks against. 


\begin{table}[h!]
	\centering
	\caption{Evaluation Models Knowledge Cutoff}
	\label{tab:model_cutoff}
	\scriptsize
	\vspace{-1em} 
	
	\begin{tabular}{p{3.6cm} p{1.2cm} p{1.2cm}}
		\toprule
		\textbf{Model} & \textbf{Knowledge-Cutoff} & \textbf{Public-Release}  \\
		\hline
		\texttt{gpt-5} & Sep 2024 & Aug 2025  \\
		\hline
		\texttt{gpt-5-mini} & May 2024 & Aug 2025  \\
		\hline
		\texttt{gpt-5-nano} & May 2024 & Aug 2025  \\
		\hline
		\textbf{\texttt{gpt-oss-20b}} & Jun 2024 & Aug 2025  \\
		\hline
		\textbf{\texttt{gpt-oss-120b}} & Jun 2024 & Aug 2025 \\
		\hline
		\texttt{gemma-3-1b-it} & Aug 2024 & Mar 2025 \\
		\hline
		\texttt{gemma-3-4b-it} & Aug 2024 & Mar 2025 \\
		\hline
		\texttt{gemma-3-12b-it} & Aug 2024 & Mar 2025 \\
		\hline
		\texttt{gemma-3-27b-it} & Aug 2024 & Mar 2025  \\
		\hline
		\texttt{Llama-3.2-3B-Instruct} & Dec 2023 & Sep 2024 \\
		\hline
		\texttt{Llama-3.1-8B-Instruct} & Dec 2023 & Jul 2024 \\
		\hline
		\texttt{Llama-3.3-70B-Instruct} & Dec 2023 & Dec 2024 \\
		\hline
		\texttt{Llama-4-Maverick-Instruct-FP8} &  Aug 2024 & Apr 2025  \\
		\hline
		\texttt{phi-4} & June 2024 & Dec 2024\\
		\hline
		\texttt{Qwen3-1.7B} &  & Apr 2025 \\
		\hline
		\texttt{Qwen3-4B-Instruct-2507} &  & Aug 2025 \\
		\hline
		\texttt{Qwen2.5-7B-Instruct} &  & Sep 2024  \\  
		\hline
		\texttt{Qwen3-30B-A3B-Instruct-2507} &  & Jul 2025 \\
		\hline
		\textbf{\texttt{Qwen3-235B-A22B-Instruct-2507}} &  & Jul 2025  \\
		\bottomrule
	\end{tabular}
	%\vspace{-1em} 
\end{table}


\subsection{Question rewrite procedure}

The question rewriting and expansion was approached as a pre-processing step before benchmarking the evaluation models using the modified questions. 
We evaluated three different LMs for question rewriting: small \texttt{gpt-oss-20b}, medium \texttt{gpt-oss-120b}, and large \texttt{Qwen3-235B-A22B-Instruct-2507}.
These are indicated in \Cref{tab:model_cutoff} with bold.
For each question in each dataset the rewriting LM was provided the original question, correct answer, and grounding context and prompted to (\Cref{apx:rewrite-prompt}) write the question to disambiguate what was being asked.
In addition to rewriting the question the model must generate what it thinks the correct answer should be, this serves as a validation check against topic drift.
This results in three variants of each question being generated, one for each of the rewriting models being evaluations (bold models in \Cref{tab:model_cutoff}).

With the questions rewritten, we filter them to ensure no degenerate questions are included. 
An LM-as-a-Judge is used to extract the following properties: reformatted question similarity to the original question, reformatted answer similarity to the original answer, reformatted question giveaway score, original question giveaway score.
The reformatted question and answer similarities scores are used to discard any questions which are too dissimilar to the original.
The giveaway scores are used to remove any reformatted questions which got easier in the process of rewriting. 
I.e. if the reformatted giveaway score is higher (the answer is given away more in the question) than the original question, that entry is discarded.
This produces a new benchmark for each of the rewriting models that is strictly more difficult than the original.



\subsection{Evaluation methodology}

%To evaluate the benchmark accuracy uplift provided by question rewriting and expansion 
The approach to evaluate the efficacy of query rewriting grounded in context each model under evaluation is benchmarked under three configurations.
{\renewcommand\labelenumi{(\theenumi)}
\begin{enumerate*} %packed_enum   enumerate
	\item Original Question: the normal benchmarking use case, 
	\item Original Question with Context Prepended: the RAG retrieval use case, and 
	\item Rewriten Question: the uplift imparted by rewriting.
\end{enumerate*}
}

To quantify the accuracy uplift, we compute the following average per-model and per-dataset differences.
{\renewcommand\labelenumi{(\theenumi)}
	\begin{enumerate*} 
		\item Rewriten Question - Original Question: the direct rewrite uplift, 
		\item Rewriten Question - Original Question with Context Prepended: the rewrite uplift compared to the RAG retrieval use case
	\end{enumerate*}
}
The comparison between the rewritten question and the original question with the context prepended enables evaluating whether the rewrite outperforms benchmarking models with a RAG database which contains background information.
However, if the context contains the answer, one would expect any benchmarking with the context included to yield roughly 100\% accuracy.
To mitigate this we developed a version of each grounding context that is "Answer-Free". 
This context was developed using a static LM (\texttt{gpt-oss-120b}) to rewrite the context (see \Cref{apx:afc} for the prompt).
The results section covers the accuracy uplift using the original context (the less interesting use case) as well as the Answer-Free Context (AFC).
AFC replicates the use case where a RAG system surfaced background information, but not the actual answer to the users question. 
Our results demonstrate accuracy uplift in benchmark performance when question rewriting is performed using Answer-Free Context. 
This uplift effect is larger than just prepending the AFC before the benchmark question.

To characterize one hypothesis for why this affect shows up, we leverage an embedding model to understand the change in cosine distance between the original question and the context vs cosine distance between the rewritten question and the context.
We demonstrate that improvements in benchmark accuracy correlate with reductions in the cosine distance between the question and context.
In other words, the rewritten question appears to place the model closer to the right frame of mind to answer the question, as the cosine distance between the question and the context is smaller after the rewrite.



\section{Results}
% gpt-5-mini on HLE rewrite using gpt-oss-20b
% orig = 0.139
% rewrite = 0.372
\Cref{fig:figure1} outlines the accuracy uplift for all evaluated models on just the HLE-subset dataset.
Given that \texttt{gpt-oss-20b} leveraged Answer-Free Context for question rewriting, the accuracy improvement in models like \texttt{gpt-5-mini} (from 13.9\% to ~37.2\%) is remarkable.
To validate that our Answer-Free Context is not in fact giving away the answer, we plot the performance of all models across all datasets comparing the original question accuracy to the benchmark accuracy with the context prepended before the question during benchmark evaluation. 
\begin{figure}[h!]
		\vspace*{-0.5em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/data-subset-500-gpt120b-orig-giveaway.pdf}
	\vspace{-0.8em}
	\caption{Scatterplot of benchmark accuracy values per-dataset and per-model plotting the original question accuracy against the original question prepended with the answer-containing context, resulting in significant over-performance.}
	\label{fig:orig-giveaway}
	%	\vspace{-1em}   
\end{figure}
\Cref{fig:orig-giveaway} scatterplots the original question benchmark accuracy on the x-axis vs the original question with associated context on the y-axis.
For most models and datasets prepending the answer-containing context provides significant uplift in benchmark accuracy.
In comparison, \Cref{fig:orig-giveaway-afc} scatterplots the same values but prepending the Answer-Free Context instead.
\begin{figure}[h!]
	%	\vspace{-1.0em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/data-subset-500-afc-gpt120b-orig-giveaway.pdf}
	\vspace{-0.8em}
	\caption{Scatterplot of benchmark accuracy values per-dataset and per-model plotting the original question accuracy against the original question prepended with the answer-free context, resulting in no trend of accuracy improvement attributable to the context.}
	\label{fig:orig-giveaway-afc}
	%	\vspace{-1em}   
\end{figure}
This results in no pattern of accuracy uplift attributable to the AFC, indicating the the answer removal process was successful.


To compare the per-dataset per-model benchmark accuracy uplift, \Cref{fig:r_minus_q} plots the distribution in accuracy improvement measured as the rewritten question accuracy minus the original question accuracy. 
This demonstrates that models benchmark better on the rewritten question compared to the original questions, but considering the rewritten questions likely contain more detailed information drawn from context, its not a fair comparison. 
\begin{figure}[h!]
		\vspace{-0.5em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/acc_uplift_gpt120/r_minus_q.pdf}
	\vspace{-0.8em}
	\caption{Per-dataset per-model difference in benchmark accuracy between the rewritten question and the original question. The violin plot distribution highlights the range of accuracy deltas over all datasets for each model evaluated.}
	\label{fig:r_minus_q}
		\vspace{-1em}   
\end{figure}
\FloatBarrier
To demonstrate the accuracy uplift from rewriting under the worst possible conditions, \Cref{fig:r_minus_q_afc_giveaway} plots the accuracy of the rewritten questions minus the original questions with the answer-free context prepended. 
Note, the question rewriting process only used the answer-free context, so both the rewritten question and the original question with AFC are on even footing information-wise.
%Additionally, the model under evaluation has access to all of the information the rewriting model did, but the task is question answering instead of disambiguation, resulting in lower benchmark accuracy compared to a separate rewrite then answer paradigm.
Additionally, the model under evaluation has access to all information the rewriting model did. 
This demonstrates that the act of rewriting the questions in a separate rewrite-then-answer paradigm produces an accuracy improvement that is not possible by just prepending the same information into the models context during evaluation.
\begin{figure}[h!]
		\vspace{-0.5em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/acc_uplift_gpt120/rafc_minus_qafc_giveaway.pdf}
	\vspace{-0.8em}
	\caption{Violin plots of the per-dataset per-model difference in benchmark accuracy between the rewritten questions and the original questions with associated answer-free context. The violin plot distribution highlights the range of accuracy deltas over all datasets for each model evaluated.}
	\label{fig:r_minus_q_afc_giveaway}
		\vspace{-1em}   
\end{figure}

The violin plot information can be visualized using a scatterplot to break out the effects of model and dataset; where the x-axis is the original benchmark accuracy and the y-axis is the rewritten question performance. 
\Cref{fig:gpt120b-afc} showcases the results from all datasets before model knowledge cutoffs. 
All dataset and model combinations lie above the $y=x$ line which indicates identical performance between the original and rewritten questions. 
Some dataset model combinations get more uplift than others, but there are no combinations where benchmark performance decreases. 
\begin{figure}[h!]
		\vspace{-0.5em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/gpt120b-afc.pdf}
	\vspace{-0.8em}
	\caption{Scatterplot showing the per-dataset per-model improvement in benchmark accuracy caused by rewriting the question. The x-axis shows the original question benchmark accuracy while the y-axis shows the rewritten question benchmark accuracy.}
	\label{fig:gpt120b-afc}
	%	\vspace{-1em}   
\end{figure}
Additionally, some datasets like \texttt{flashrag\_fermi} demonstrate significant uplift stemming from disambiguation. 

Breaking out the benchmark accuracy uplift results per-dataset reveals an interesting exception, comparing the rewritten HLE-subset to the original questions with the answer-free context prepended demonstrates the only consistent drop in accuracy, as shown in \Cref{fig:r_minus_q_afc_giveaway_dataset}.
In \Cref{fig:r_minus_q_afc_giveaway_dataset} the first 5 datasets are from post knowledge cutoff, and the remainder before. 
Additionally, due to a paucity of datasets released since the beginning of the year, all but the HLE-subset were generativly created benchmarks using wither \cite{majurski2025generative} or \cite{shashidhar2025yourbench}.
Those benchmarks tend to have more complex questions that are less fact based than the extractive QA datasets which make up the majority. 
Thus, they demonstrate minimal benchmark accuracy uplift from rewriting compared to just prepending the answer-free context.
This trend extends to the HLE-subset which has the most complex and difficult questions over all the datasets.
Therefore, there is a trend that fact based benchmarking is improved by the question rewrite, whereas questions where you need to reason through and think about the question don't.
These trends are demonstrated in \Cref{fig:figure1} the rewritten HLE questions provide significant accuracy uplift compared to the original questions, but less than just prepending the answer-free context.
\begin{figure}[h!]
	\vspace{-0.5em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/acc_uplift_gpt120/rafc_minus_qafc_giveaway_dataset.pdf}
	\vspace{-0.8em}
	\caption{Violin plots of the per-dataset per-model difference in benchmark accuracy between the rewritten questions and the original questions with associated answer-free context. The violin plot distribution highlights the range of accuracy deltas over all models for each dataset evaluated.}
	\label{fig:r_minus_q_afc_giveaway_dataset}
	\vspace{-1em}   
\end{figure}

Our hypothesis is that the benchmark accuracy improvement from reformatting the questions with answer-free context stems from disambiguation and improving the concept alignment between the question and the underlying context information.
To demonstrate this we plot the improvement in benchmark accuracy between the rewritten question and original question against the improvement in cosine similarity between the question and context when questions are rewritten.
\Cref{fig:acc_vs_embedding_gpt120b_e5-mistral-7b-instruct} demonstrates that most scattterplot points end up in the upper right quadrant, where improvements in benchmark accuracy correlate with improvements in cosine similarity between the question and context.
In other words, rewritten questions that have a higher cosine similarities to the context also have a higher benchmark accuracy.
Its worth noting that for every model and dataset combination pre-knowledge cutoff these points all landed in the upper right quadrant.
However, for some models in the post-knowledge cutoff datasets there is a drop in benchmark accuracy as evidenced by those points in the upper left quadrant. 
Each dataset has a distribution of accuracy improvements across the evaluated models, which is why the datasets in \Cref{fig:acc_vs_embedding_gpt120b_e5-mistral-7b-instruct} forms horizontal bands.
\begin{figure}[h!]
	\vspace{-0.5em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/acc_vs_embedding_gpt120b_e5-mistral-7b-instruct.pdf}
	\vspace{-0.8em}
	\caption{Scatterplot plotting the benchmark accuracy improvement on the x-axis vs improvement in cosine similarity between the question and context for rewritten questions on the y-axis. The x-axis plots delta benchmark accuracy Rewrite\_Q - Orig\_Q. Larger values indicate the rewritten question improved benchmark accuracy. The y-axis plots the increase in cosine similarity between the question and context resulting from rewriting. Larger values indicate better alignment between Rewrite\_Q and AFC compared to Orig\_Q and AFC. That all points are in the upper right quadrant indicates that improvements in embedding alignment between the question and grounding context correlates with improvements benchmark accuracy.}
	\label{fig:acc_vs_embedding_gpt120b_e5-mistral-7b-instruct}
	%	\vspace{-1em}   
\end{figure}


If question rewriting has a positive impact on benchmark accuracy above and beyond using the answer-free context to ground the response, could that be incorporated into the CoT prompting?
\Cref{fig:insitu_rafc_minus_q_afc_giveaway_dataset} demonstrates that attempting to move this rewrite operations into benchmark evaluation as a single operation does not produce the accuracy uplift. 
This holds both for models with \texttt{<thinking>} support and those without. 
This was evaluated by using the same question rewriting prompt lightly modified to tell the LM under evaluation to first rewrite the question for disambiguation before answering the rewritten question using the same formatting as all other LM benchmarking used in this study. 
\Cref{fig:insitu_rafc_minus_q_afc_giveaway_dataset} compares the Insitu\_Rewrite\_Q to the Orig\_Q with AFC, so the same information is presented to both evaluation runs, the only difference is in the prompting directions to first rewrite instead of just directly answer. 
\begin{figure}[h!]
	\vspace{-0.5em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/insitu_rafc_minus_q_afc_giveaway_dataset.pdf}
	\vspace{-0.8em}
	\caption{Per-dataset improvement in benchmark accuracy from performing an insitu-rewrite of the question using answer-free context during benchmark evaluation. This combines the rewrite-then-answer into a single operation. The prior accuracy improvement disappears highlighting the impact of task separation between the rewrite and answer phases.}
	\label{fig:insitu_rafc_minus_q_afc_giveaway_dataset}
	%	\vspace{-1em}   
\end{figure}


\subsection{Limitations}

The evaluation of the impact of question rewriting relies heavily on extractive QA datasets which have paired question and context tuples.
The exception is the HLE-subset where the questions are post-facto grounded using internet search.
Thus many of the fact-based extractive QA dataset questions are easily answerable when the LM is presented the answer-containing context. 
This limitation is primarily a result of the availability of public dataset of questions paired with grounding context.

\section{Conclusion}

When provided grounding context without the correct answer, LM benchmarking accuracy can be improved by following a similar question disambiguation rewriting policy that information retrieval literature has been leveraging.
This question rewriting does not require that the dynamic context system (i.e. RAG) has found and surfaced the answer to the users question, only that is discovered relevant background information. 
Comparing the rewritten question benchmark accuracy to the original question with associated answer-free context prepended reveals strong uplift attributable to the rewriting process, and not just the inclusion of relevant information into the context window. 
 Additionally, the separation of tasks into rewrite-then-answer provides accuracy improvements that prompt modification alone cannot (\Cref{fig:insitu_rafc_minus_q_afc_giveaway_dataset}). 
 This holds true for models with and without formalized "reasoning" capability. 
 
 A notable exception to this uplift is the comparison between rewritten questions and original questions with AFC for the more complex questions in the generative benchmark dataset and the HLE-subset.
 \Cref{fig:r_minus_q_afc_giveaway_dataset} demonstrates that for these highly detailed and complex questions, simply prepending the answer-free context provides equivalent or better performance improvements to attempting to rewrite the questions. 
 Considering the answer-free context contains significantly more background information than the rewritten question can possible contain, perhaps it isn't to surprising the rewritten question alone cannot provide the full uplift that AFC context can, especially for competent models which can reason over that context.
 
 Interestingly enough, the rewritten question with AFC usually out-performs the original question with AFC. 
 \Cref{fig:rafc_giveaway_minus_qafc_giveaway_dataset} highlights that accuracy uplift, with the Fermi reasoning estimation dataset demonstrating significant uplift. 

\begin{figure}[h!]
	\vspace{-0.5em}
	\centering
	\includegraphics[width=0.9\columnwidth]{figs/acc_uplift_gpt120/rafc_giveaway_minus_qafc_giveaway_dataset.pdf}
	\vspace{-0.8em}
	\caption{Per-dataset improvement in benchmark accuracy from the Rewrite\_Q with AFC compared to the Orig\_Q with AFC during benchmark evaluation. This highlights that both the question rewriting and the addition of the answer-free context improve benchmark accuracy.}
	\label{fig:rafc_giveaway_minus_qafc_giveaway_dataset}
	%	\vspace{-1em}   
\end{figure}





%\subsection{Future work}
%TODO: 
%- characterize the change in question length from orig to rewrite. I.e. how many tokens are being added to disambiguate. 
%- measure the alignment and utility of the context (orig and afc) w.r.t. the question. I.e. does the improvement in answer accuracy correlate with the context utility? i.e. does the reduction in accuracy from orig+giveaway (below the trendline) stem from useless information being included? I.e. would the post-cutoff trend above the trendline return if we only considered those questions where the context paragraph has known utility?



\newpage
\bibliographystyle{plainnat}
{\small
	\bibliography{references}
}

\newpage
\appendix
\input{appendix}


\end{document}




